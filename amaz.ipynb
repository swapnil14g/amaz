{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14K3mlzpt42GdNMo3akojYz0W9rzgxUee",
      "authorship_tag": "ABX9TyNBJlN+19+1ED3w7eMO3vtH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnil14g/amaz/blob/main/amaz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyVHSNVl9Vr1",
        "outputId": "ade48976-b6c6-4c2b-ff8d-eba4b4598956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263859/263859 [41:22<00:00, 106.30it/s]\n",
            " 37%|███▋      | 97526/263859 [45:58<1:11:43, 38.65it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/src/')\n",
        "\n",
        "# Import custom modules\n",
        "from utils import download_images, parse_string\n",
        "from constants import entity_unit_map, allowed_units\n",
        "\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/dataset/train.csv'\n",
        "download_folder = \"Images\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(download_folder):\n",
        "    os.makedirs(download_folder)\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(csv_file_path)\n",
        "image_links = df['image_link'].tolist()\n",
        "\n",
        "# Download images\n",
        "download_images(image_links, download_folder, allow_multiprocessing=True)\n",
        "# Set up paths\n",
        "DATASET_FOLDER = '/content/drive/MyDrive/Colab Notebooks/dataset/'\n",
        "IMAGE_FOLDER = '/content/drive/MyDrive/Colab Notebooks/Images/'\n",
        "OUTPUT_FILE = 'test_out.csv'\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
        "\n",
        "# Download images (uncomment when ready to download)\n",
        "download_images(train_df['image_link'], IMAGE_FOLDER)\n",
        "download_images(test_df['image_link'], IMAGE_FOLDER)\n",
        "\n",
        "# Custom dataset class\n",
        "class ProductImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, os.path.basename(self.dataframe.iloc[idx]['image_link']))\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        entity_name = self.dataframe.iloc[idx]['entity_name']\n",
        "        if 'entity_value' in self.dataframe.columns:\n",
        "            entity_value = self.dataframe.iloc[idx]['entity_value']\n",
        "            number, unit = parse_string(entity_value)\n",
        "            return image, entity_name, number, unit\n",
        "        else:\n",
        "            return image, entity_name\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProductImageDataset(train_df, IMAGE_FOLDER, transform=transform)\n",
        "test_dataset = ProductImageDataset(test_df, IMAGE_FOLDER, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define model\n",
        "class EntityExtractionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EntityExtractionModel, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Identity()\n",
        "        self.fc1 = nn.Linear(num_ftrs, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.fc3 = nn.Linear(256, 1)  # for numeric value\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        unit = self.fc2(x)\n",
        "        value = self.fc3(x)\n",
        "        return unit, value\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = len(allowed_units)\n",
        "model = EntityExtractionModel(num_classes)\n",
        "criterion_unit = nn.CrossEntropyLoss()\n",
        "criterion_value = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, entity_names, numbers, units in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        numbers = numbers.to(device).float()\n",
        "        units = torch.tensor([list(allowed_units).index(u) for u in units]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        unit_pred, value_pred = model(images)\n",
        "        loss_unit = criterion_unit(unit_pred, units)\n",
        "        loss_value = criterion_value(value_pred.squeeze(), numbers)\n",
        "        loss = loss_unit + loss_value\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Prediction on test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, entity_names in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        unit_pred, value_pred = model(images)\n",
        "        unit_pred = torch.argmax(unit_pred, dim=1)\n",
        "        unit_pred = [list(allowed_units)[i] for i in unit_pred.cpu().numpy()]\n",
        "        value_pred = value_pred.squeeze().cpu().numpy()\n",
        "\n",
        "        for value, unit in zip(value_pred, unit_pred):\n",
        "            predictions.append(f\"{value:.2f} {unit}\")\n",
        "\n",
        "# Create output file\n",
        "test_df['prediction'] = predictions\n",
        "test_df[['index', 'prediction']].to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"Output saved to {OUTPUT_FILE}\")\n",
        "\n",
        "# Run sanity check\n",
        "os.system(f\"python src/sanity.py --test_filename {os.path.join(DATASET_FOLDER, 'test.csv')} --output_filename {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83W2ecvw9egh",
        "outputId": "33e64b56-3906-4d4c-efae-c56de14889c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}